---
title: "Hotel Reviews"
author: "Zhichao Hu, Hui Wang, and Junjie Yang"
date: "November 30, 2019"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(lubridate)
library(ggthemes)
library(faraway)
library(lmtest)
library(MASS)
library(leaps) ## for regsubsets()
```

# EDA of the dataset
## load data
```{r}
hotel_reviews <- read_csv("Hotel_Reviews.csv")
```

## date range of all reviews
```{r}
min(mdy(hotel_reviews$Review_Date))
max(mdy(hotel_reviews$Review_Date))
```

## unique hotels in dataset
There are two hotel with the same name but different addresses; address should be used as a unique identifier.
```{r}
hotel_reviews$Hotel_Name %>% unique() %>% length()
hotel_reviews$Hotel_Address %>% unique() %>% length()
```

## hotels with most number of reviews
```{r}
hotel_reviews %>% group_by(Hotel_Address) %>% summarise(name=first(Hotel_Name), count=n()) %>% 
  arrange(desc(count)) %>% select(name, count)

```

## summarise related columns to create predictors
```{r}
hotel_agg <- hotel_reviews %>% group_by(Hotel_Address) %>% 
  summarise(name=first(Hotel_Name), 
            review_t=mean(Total_Number_of_Reviews), # t stands for total
            review_n=mean(Review_Total_Negative_Word_Counts), # n stands for negtive
            review_p=mean(Review_Total_Positive_Word_Counts), # p stands for positive
            review_m=mean(Additional_Number_of_Scoring), # m stands for missing
            reviewer_exp=mean(Total_Number_of_Reviews_Reviewer_Has_Given), #exp stands for experience
            score=mean(Average_Score))
colnames(hotel_agg)[1] <- "address"
```

# Descriptive satatistics.
```{r}
dim(hotel_agg)
summary(hotel_agg)
sd(hotel_agg$score)
```

## score histgram

The average scores are not prefectly normal.
```{r}
hotel_agg %>% ggplot(aes(score))+geom_histogram(aes(y=..density..), binwidth=0.1, color="blue", fill="light blue")+
  geom_density(color="red", size=1, fill="red", alpha=0.2)+theme_bw()
```

## remove an outlier with the score of 5.2

```{r}
OutVals = boxplot(hotel_agg$score)$out
OutVals
which(hotel_agg$score < 5.5)
hotel_agg1=hotel_agg[ -which(hotel_agg$score < 5.5), ] # remove the #1444 hotel with the score of 5.2

hotel_agg1 %>% ggplot(aes(score))+geom_histogram(aes(y=..density..), binwidth=0.1, color="blue", fill="light blue")+
  geom_density(color="red", size=1, fill="red", alpha=0.2)+theme_bw()
dim(hotel_agg1)
```

Two predictors, review_t and review_m, are not normal distributed. Both variables are related to amount of reviews.

```{r}
par(mfrow = c(2,3))
hist(hotel_agg1$review_t, main = NULL, ylab = NULL)
hist(hotel_agg1$review_n, main = NULL, ylab = NULL)
hist(hotel_agg1$review_p, main = NULL, ylab = NULL)
hist(hotel_agg1$review_m, main = NULL, ylab = NULL)
hist(hotel_agg1$reviewer_exp, main = NULL, ylab = NULL)
```

# linear model of averaged predictors
## build linear model
Average score of each hotel is the response.
```{r}
lmod0 <- lm(score~review_t+review_n+review_p+review_m+reviewer_exp, hotel_agg1)
summary(lmod0)
```

## model diagnosis

Try to identify a transformation using Boxcox. The nearest integer is 3, so we consider a transformation of the fitted value with lambda = 3.

```{r}
boxcox(lmod0, lambda = seq(1,5,0.1), plotit = TRUE)
scoreCube = hotel_agg1$score*hotel_agg1$score*hotel_agg1$score  # Box-cox transformation
review_t1 = hotel_agg1$review_t^(1/3)  # To make it normal distributed
review_m1 = hotel_agg1$review_m^(1/3)  # To make it normal distributed
lmod_tf = lm(scoreCube ~ review_t1 + review_n + review_p + review_m1 + reviewer_exp, hotel_agg1)
summary(lmod_tf)

# 是否需要transform predictors
par(mfrow = c(2,3))
hist(review_t1, main = NULL, ylab = NULL)
hist(hotel_agg1$review_n, main = NULL, ylab = NULL)
hist(hotel_agg1$review_p, main = NULL, ylab = NULL)
hist(review_m1, main = NULL, ylab = NULL)
hist(hotel_agg1$reviewer_exp, main = NULL, ylab = NULL)
```

### check error assumption
#### constant variance assumption

Plot the residuals against fitted value and predictors.
Choose review_t and review_m, to take F-test and reject the null hypothesis based on the p-value.
Review_t and review_m are related to the number of reviews or scores of hotels. The hotels who have a small amout of reviwes have larger residuals. 

The other residuals variances are constant. 
```{r}
par(mfrow=c(2,3))
plot(lmod0$fitted.values,residuals(lmod0))
abline(h=0, col = 'red')
plot(hotel_agg1$review_t,residuals(lmod0))
abline(h=0, col = 'red')
plot(hotel_agg1$review_n,residuals(lmod0))
abline(h=0, col = 'red')
plot(hotel_agg1$review_p,residuals(lmod0))
abline(h=0, col = 'red')
plot(hotel_agg1$review_m,residuals(lmod0))
abline(h=0, col = 'red')
plot(hotel_agg1$reviewer_exp,residuals(lmod0))
abline(h=0, col = 'red')

var.test(residuals(lmod0)[hotel_agg1$review_t>1000], residuals(lmod0)[hotel_agg1$review_t<1000])
var.test(residuals(lmod0)[hotel_agg1$review_p>28], residuals(lmod0)[hotel_agg1$review_p<28])
var.test(residuals(lmod0)[hotel_agg1$review_m>200], residuals(lmod0)[hotel_agg1$review_m<200])
```

#### normality assumption

The QQ-plot looks heavy-tailed and the p-value of Shapiro-Wilk normality test is smaller than 0.05.
But from the histgram plot, it looks normal.

```{r}
qqnorm(residuals(lmod0),ylab="Residuals",main="QQ plot")
qqline(residuals(lmod0),col="blue")
shapiro.test(residuals(lmod0))

hist(residuals(lmod0), breaks = 50)
```

#### correlated errors

Do we still need to check the correlated errors? 

(The p-value of DW test is much smaller than 0.05, we need reject the null hypothesis. )

```{r}
dwtest(scoreCube~review_t1+review_n+review_p+review_m1+reviewer_exp, data = hotel_agg)
```

### multicolliearity - Variance Inflation Factor

The max Variance Inflation Factor is smaller than 10 and the average of VIF is less than 3. We think the predictors are not multicolliearity.

```{r}
x = model.matrix(lmod0)[, -1]
vif(x)
mean(vif(x))
```

Outlier

```{r}
nrow(hotel_agg1)
stud = rstudent(lmod0)
jackres = stud*(1485/(1486-stud^2))^0.5
head(jackres[order(abs(stud), decreasing = T)])
qt(.025/1492, 1485)
```

### Influential Observations - Cook's D

It appears that all Cook's D values are smaller than 1. We remove the obervation #1424 and creat a new model as lmod1. Because we consider it is a influential data point as its Cook's D value is much higher than the one follows.

```{r}
cook = cooks.distance(lmod0)
halfnorm(cook, 1, ylab = "Cook's distance")

lmod1 = lm(score~review_t+review_n+review_p+review_m+reviewer_exp, hotel_agg1, subset = (cook < 0.3))
summary(lmod1)
```

# model selection

##forward search

```{r}
n = nrow(hotel_agg1)
out.null = lm(score ~ 1, hotel_agg1)
out.full = lmod1
full = formula(lmod1)
out_f = regsubsets(full, data = hotel_agg1, method = 'forward', nvmax = 5)
out_f0 = summary(out_f)
out_f0$outmat
```

```{r}
bic = out_f0$bic
a = c(1, 2, 3, 4, 5)
plot(a, bic)
bic
adjr2=out_f0$adjr2
adjr2
plot(out_f, scale="adjr2", main="Forward Selection: adjr2")

which.min(model_fwd_summary$bic) 
which.max(model_fwd_summary$adjr2)
#Best model in terms of both adjusted R-squared and bic is the full model 

```

##Early stopping forward selection using BIC for model lmod1

```{r}
out.forward = step(out.null, scope = list(lower = ~1, upper = full), 
                   k = log(n), direction = "forward", trace = FALSE)
out.forward$coefficients
```

## 5 Fold Cross Validation for lmod1 (full model)
```{r}
set.seed(1)
k=5
fold=sample(1:k,nrow(hotel_agg1),replace=TRUE)

kfold.rmse=1:k # we will have 5 RMSEs to fill this with later
for(i in 1:k){
  test=hotel_agg1[fold==i,] # test set is the ith group for the ith iteration
  train=hotel_agg1[fold!=i,]# training set is all the other groups
  
  pred.time=predict(lmod1,test)
  rmse=sqrt(mean((test$score-pred.time)^2))
  
  kfold.rmse[i]=rmse # store current iteration RMSE into ith position of kfold.rmse
}

kfold.rmse # show our RMSEs for each iteration
mean(kfold.rmse) #avg rmse 0.3525199
```

## 5 Fold Cross Validation for lmodnoexp (reduced model using early stopping)
```{r}
#If we ddecide to get rid of variable exp based on forward selection
lmodnoexp = lm(score~review_t+review_n+review_p+review_m, hotel_agg1, subset = (cook < 0.3))
summary(lmodnoexp)
set.seed(1)
k=5
fold=sample(1:k,nrow(hotel_agg1),replace=TRUE)

kfold.rmse=1:k # we will have 5 RMSEs to fill this with later
for(i in 1:k){
  test=hotel_agg1[fold==i,] # test set is the ith group for the ith iteration
  train=hotel_agg1[fold!=i,]# training set is all the other groups
  
  pred.time=predict(lmodnoexp,test)
  rmse=sqrt(mean((test$score-pred.time)^2))
  
  kfold.rmse[i]=rmse # store current iteration RMSE into ith position of kfold.rmse
}

kfold.rmse # show our RMSEs for each iteration
mean(kfold.rmse) #avg rmse 0.353181
```


# liner model of a single hotel
## Prepare subset
Choose a hotel with most number of reviews to explore other individual characteristics of reviewers as predictors
```{r}
#subset the intial dataframe to a single hotel
hotel_bri <- hotel_reviews %>% filter(Hotel_Name=="Britannia International Hotel Canary Wharf") %>% 
  select(review_t=Total_Number_of_Reviews,
         review_n=Review_Total_Negative_Word_Counts,
         review_p=Review_Total_Positive_Word_Counts,
         review_m=Additional_Number_of_Scoring,
         reviewer_exp=Total_Number_of_Reviews_Reviewer_Has_Given,
         score_i=Reviewer_Score) # i stands for individual

```

## individual score histgram

The individual scores are not normal.
```{r}
hotel_bri %>% ggplot(aes(score_i))+geom_histogram(aes(y=..density..), binwidth=0.1, color="blue", fill="light blue")+
  geom_density(color="red", size=1, fill="red", alpha=0.2)+theme_bw()
```

## build linear model
Each individual score (Reviewer_Score) is the response.
```{r}
lmod_bri <- lm(score_i~review_t+review_n+review_p+review_m+reviewer_exp, hotel_bri)
```

```{r}
summary(lmod_bri)
```

```{r}

```


